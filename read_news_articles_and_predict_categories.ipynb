{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdQ07Tk21F/e7Nc2k5CJHr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipratiwari/Algorithems/blob/master/read_news_articles_and_predict_categories.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSY8KWScc1T0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will be using Dask to load and preprocess a large dataset of online news articles, and then using Scikit-Learn to train a machine learning model to predict the category of the article based on its content."
      ],
      "metadata": {
        "id": "JqWqcL7yc54H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask scikit-learn pandas nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUaLHvCXc7Ny",
        "outputId": "ca8b2d61-a905-4fb9-aea8-6522e333e12f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.9/dist-packages (2022.12.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask) (2.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask) (6.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from dask) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask) (8.1.3)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask) (2023.3.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask) (0.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.9/dist-packages (from partd>=0.3.10->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "dugpMwHFdam1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Download dataset\n",
        "!wget https://storage.googleapis.com/dask-tutorial-data/notebook-data/online-news-popularity/online-news-popularity.csv\n",
        "\n",
        "# Load dataset with Dask\n",
        "df = dd.read_csv('online-news-popularity.csv')"
      ],
      "metadata": {
        "id": "78LgnHPhdbOL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset"
      ],
      "metadata": {
        "id": "_Pb7cWvyfzJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenize the article content and remove stop words\n",
        "def tokenize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['content'].apply(tokenize)\n",
        "\n",
        "# Create bag-of-words features\n",
        "cv = CountVectorizer(max_features=10000)\n",
        "X = cv.fit_transform(df['tokens'].compute().apply(' '.join))\n",
        "y = df['shares']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-uJdg7fffzqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Scikit-Learn model"
      ],
      "metadata": {
        "id": "dya_gfc0gCL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "kgR50GD2dW1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using Dask for parallel processing"
      ],
      "metadata": {
        "id": "pJyNgahBgKt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dask_ml.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'normalize': [True, False]}\n",
        "grid_search = GridSearchCV(model, parameters, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GbY_x6LtdQy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model on the test set"
      ],
      "metadata": {
        "id": "GXpxSv8QgUkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "y_pred = grid_search.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'R-squared: {r2:.2f}')"
      ],
      "metadata": {
        "id": "D1ex-_sEdQ11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwmvcdghdQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95b9DF-wdQ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAjl6yuodQ-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RH-DVmdgdRBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}